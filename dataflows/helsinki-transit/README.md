
# Helsinki Transit Dataflow

The Helsinki transit dataflow uses MQTT to ingests data from the Helsinki transit API and then processes the data to provide insights into the transit system. The purpose of the dataflow is to demonstrate how to use the SQL interface to analyze the data.

The following diagram is a visual representation generated by `sdf`:

<p align="center">
 <img src="img/helsinki-transit.jpg">
</p>

The dataflow has two services:
* `clean-events` removes events with incomplete data
* `generate-vehicle-stats` generates average speed per vehicle every 5 seconds.


## Step-by-step

Take a look at the [dataflow.yaml](./dataflow.yaml) to get an idea of what we're doing.

Make sure to [Install SDF and Start a Cluster].

### 1. Run the Dataflow

Use `sdf` command line tool to run the dataflow:

```bash
sdf run --ui
```

* Use `--ui` to generate the graphical representation and run the Studio.
* Using `sdf run` as opposed to `sdf deploy` will run the dataflow with an ephemeral worker, which will be cleaned up on exit.


### 2. Download Jaq SmartModule

To pre-process helsinki events, we need to download jaq smartmodule. this only needs to be done once:

```
$ make download-sm
```

### 3. Run MQTT connector 

MQTT connector is used to get data from city of helsinki thru their MQTT broker.

To start MQTT connector:

```
make cdk-start
```

You can inspect events by: `fluvio consume helsinki`.

For additional context, checkout [connectors](./connectors/).

You can shutdown the connector by running:

```
make cdk-shutdown
```


### 3. Check the results

In the `sdf` terminal, checkout the state maintained by the dataflow:

```bash
show state
```

We are interested in the following state:

```bash
show state generate-vehicle-stats/vehicle-stat/state
```

#### Use SQL to query the state

Let's say we want to analyze the top 10 vehicles by average speed. We can use the SQL interface to query the state.

To enter the SQL interface, run:

```bash
>> sql
```

Let's start by showing the tables:

```sql
show tables
```

There are two tables, but in this example we are interested in the `license_plates` table.
Let's run a simple select:

```sql
select * from vehicle_stat
```

Now we can look-up top 5 vehicles by speed:

```sql
select * from vehicle_stat order by speed desc limit 5
```

When you are done, exit the SQL interface with `.exit`.


#### Check the output topic

In our dataflow example, we are producing the average speed for all vehicles to the `average-speed` topic.

```bash
fluvio consume average-speed -O json
```

If we decide to change the dataflow to give us the top 5 vehicles by speed, all we need to do is update the SQL statement in the `collect_vehicle_stats` function:

```rust
let vs = sql("select * from vehicle_stat order by speed desc limit 5")?;
```

Then run the service again using `sdf run`.  Checking the output again, you will notice that the output has changeed. It only returns the top 5 vehicles by speed:

```bash
fluvio consume average-speed -O json
```

:tada: Congratulations! You have just learned how to use the SQL interface to analyze the data and improve your dataflow based on your analysis.



### Clean-up

Exit `sdf` terminal and clean-up. The `--force` flag removes the topics:

```bash
sdf clean --force
```

Stop the connector:

```bash
make cdk-cleanup
```

## Running with MCP Client

In order to access data using MCP client, you need to set up [Worker](https://www.fluvio.io/sdf/deployment) or use cloud worker.

### To run the dataflow locally

Make sure you quit out of `sdf run`.

Deploy dataflow on local worker:

```
$ sdf deploy
```

This will output from local connector that have been deployed in previously.


### Deploying to Cloud Worker

In order to deploy to cloud worker, make sure you have set up [cloud](https://www.fluvio.io/docs/cloud/quickstart).

The cloud setup should have setup the cloud profile for you. Ensure you have changed to cloud profile:

```
$ fluvio profile list
```

#### Set up cloud connectors

Download smartmodule:

```
make download-sm
```

#### Deploy Dataflow

Same as local worker, deploy dataflow 

```
$ sdf deploy
```

#### Clean update

In the cloud, running MQTT will consume lots of credits.  To shutdown connector:

```

```


### 
[Install SDF and Start a Cluster]: /README.MD#prerequisites